{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook requires a https://github.com/cocktailpeanut/dalai dalai server running"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download and unpack dalaipy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "processing_path = \"../../data/question_answering/\"\n",
    "os.makedirs(processing_path, exist_ok=True)\n",
    "\n",
    "!wget https://github.com/wastella/dalaipy/archive/refs/heads/main.zip -P {processing_path}\n",
    "unzip_path_extractor = processing_path + \"main.zip\"\n",
    "!unzip {unzip_path_extractor} -d {processing_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move main file to current directory and import it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../../data/question_answering/dalaipy-main/src/main.py daiaipy.py\n",
    "from  daiaipy import Dalai\n",
    "model = Dalai()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**function used for question generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def get_question(request):\n",
    "    # used for testing\n",
    "    # request = request[:40]\n",
    "    # used for testing\n",
    "\n",
    "    request_sentence =\"The generated question should be answerable only by the information provided in the sentence. Fill the last field:\\nAnswer: The capital of France is Paris. Generated question: What is the capital of France?\\nAnswer: Green Dragon: The Green Dragon was a British pub. Generated question:What was the Green Dragon?\\nAnswer: The movie was directed by Steven Spielberg. Generated question: Who directed the movie?\\nAnswer: The temperature is 25 degrees Celsius. Generated question: What is the temperature in Celsius?\\nAnswer: \"\n",
    "    request_sentence = request_sentence + request + \" Generated question:\"\n",
    "    #request_sentence = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n### Instruction:\\n>Ask a question that can be answered by the following sentences and tell me what the answer in the sentence is.:\\n### Input:\\n>\"\n",
    "    #request_sentence = request_sentence + request + \"\\n### Response:\"\n",
    "\n",
    "    request_dict = model.generate_request(request_sentence, \"alpaca.7B\")\n",
    "    str = model.generate(request_dict)['response'].replace(\"\\r\", \"\")[len(request_sentence):-7]\n",
    "    time.sleep(4)\n",
    "    return str.split(\"?\")[0] + \"?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_question(\"Lobalug: The Lobalug was an aquatic magical beast found at the bottom of the North Sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle('../../data/dataframes/harrypotter_pages_current_cleaned.pickle')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "model_name = \"allenai/t5-small-squad2-question-generation\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = model.generate(input_ids, **generator_args)\n",
    "    output = tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['questions'] = df['text'].apply(run_model)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "for i in np.random.randint(len(df), size= 10):\n",
    "    print(str(i) + \" - \" +  \": \")\n",
    "    print(df.iloc[i][\"text\"])\n",
    "    print(\"-----------question----------------\")\n",
    "    print(df.iloc[i][\"questions\"])\n",
    "    print(\"------------------------------------------\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
