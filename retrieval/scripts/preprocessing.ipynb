{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**used libraries:**\n",
    "- pandas\n",
    "- glob\n",
    "- os \n",
    "- pyunpack\n",
    "- shutil\n",
    "- numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make a new folder in data for preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "processing_path = \"../../data/preprocessing/\"\n",
    "os.makedirs(processing_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clone and unzip wikiextractor**\n",
    "- **wikiextractor has to be cited in the paper! for citing information see github page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/attardi/wikiextractor/archive/master.zip -P {processing_path}\n",
    "unzip_path_extractor = processing_path + \"master.zip\"\n",
    "!unzip {unzip_path_extractor} -d {processing_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**download the data dump**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elder scrolls: https://s3.amazonaws.com/wikia_xml_dumps/e/el/elderscrolls_pages_current.xml.7z\n",
    "# wiki/Special:Statistics\n",
    "\n",
    "download_link = \"https://s3.amazonaws.com/wikia_xml_dumps/h/ha/harrypotter_pages_current.xml.7z\"\n",
    "filename = download_link.split(\"/\")[-1][:-3]\n",
    "\n",
    "!wget  {download_link} -P {processing_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unpack the data dump**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyunpack import Archive\n",
    "\n",
    "Archive(processing_path + filename + \".7z\").extractall(processing_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use wikiextractor to clean the data**\n",
    "- cleaned data will be saved in json in `../../data/preprocessing/text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = processing_path + filename\n",
    "cleaned_path = processing_path + \"text\"\n",
    "!mkdir {cleaned_path}\n",
    "!python3 -m wikiextractor.WikiExtractor --json -o {cleaned_path} {path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create one dataframe from all data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# default output directory is ../../data/preprocessing/text \n",
    "\n",
    "for x in os.walk(cleaned_path):\n",
    "    for y in glob.glob(os.path.join(x[0], '**')):\n",
    "        if not os.path.isdir(y):\n",
    "            df = pd.concat([df, pd.read_json(y, lines=True)], ignore_index=True, sort=False)       \n",
    "\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For some wikis there are redirect pages which do not have any text or have weird structure. Drop them and reset index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.text != \"\"]\n",
    "df = df[df.text.str.contains(\"&lt\") == False].reset_index()\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at some example texts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.random.randint(len(df), size= 10):\n",
    "    print(str(i) + \" - \"+ df.iloc[i][\"title\"] +  \": \")\n",
    "    print(df.iloc[i][\"text\"])\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete unnecessary data and save dataframe as .pickle file**\n",
    "- dataframe can be read with `pd.read_pickle('../../data/dataframes/-filename-.pickle')` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(processing_path)\n",
    "os.makedirs(processing_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = \"../../data/dataframes/\"\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "df.to_pickle(saving_path +filename[:-4] +'.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_pickle(\"../../data/dataframes/harrypotter_pages_current.pickle\")\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to split dataframe into**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(df_prev, url, title, text):\n",
    "    split = text.split(\"\\n\")\n",
    "    arr = [title + \": \"] + split\n",
    "    if len(arr) % 2 != 0:\n",
    "        arr = arr[:-2]\n",
    "\n",
    "    arr1 = arr[0::2]\n",
    "    arr2 = arr[1::2]\n",
    "\n",
    "    # hard limit for incorrectly formatted texts\n",
    "    limit = 30\n",
    "\n",
    "    res = [x.replace(\".\", \": \") + y for x,y in zip(arr1, arr2) if len(x) < limit]\n",
    "    for j in range(1,len(res)):\n",
    "        res[j] = title + \", \" + res[j]\n",
    "    url_arr = [url] * len(res)\n",
    "    dict_list = {'URL':url_arr,'text':res}\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df = pd.concat([df, df_prev], ignore_index=True, sort=False)\n",
    "\n",
    "    return df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cleaned_df(df):\n",
    "    df_res = pd.DataFrame()\n",
    "    for i in range(len(df)):\n",
    "        df_res = split_text(df_res, df[\"url\"].iloc[i], df[\"title\"].iloc[i], df[\"text\"].iloc[i])\n",
    "\n",
    "    return df_res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clean dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = create_cleaned_df(df2)\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.randint(len(df_cleaned), size= 10):\n",
    "    print(str(i) + \" - \" +  \": \")\n",
    "    print(df_cleaned.iloc[i][\"text\"])\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**save cleaned dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = \"../../data/dataframes/\"\n",
    "os.makedirs(saving_path, exist_ok=True)\n",
    "df_cleaned.to_pickle(saving_path +filename[:-4] + \"_cleaned\"+'.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
