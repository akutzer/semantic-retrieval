## :heavy_check_mark: 1. Fandom crawl
:heavy_check_mark: Download and extract fandom dumps. \
:heavy_check_mark: Clean up these dumps, i.e. remove wiki markdown syntax, remove tables, etc., using [WikiExtractor](https://github.com/attardi/wikiextractor)

:heavy_check_mark: Remove some pages, like the front page, and whatever else you can find. Update the passage RegEx pattern, which filters out passages, such as one of the type `__[\w*]__` or whatever.

:heavy_check_mark: Split the page content into paragraphs, by first splitting those paragraphs into sentences using sentence tokenizers. Then split the Sentences into words (using a [WordPieceTokenizer](https://huggingface.co/docs/tokenizers/api/models#tokenizers.models.WordPiece)) to get it's length. After that start merging the sentences until a certain maximal passage length is reached, then start a new passage. This passage length should be a hard limit, so no passage is longer than it. If a single sentence is longer than the maximum passage length, we have to split it in half.

:heavy_check_mark: If passages or paragraphs are very short, merge them together.

Save the final dump in a JSON with the following format:
```json
[
    {
        "id": int,
        "revid": int,
        "url": string,
        "title": string,
        "text": list(string)
    },
    ...
]
```

If we have time we might also download and extract multi-linguistic wikis.


## 2. Generate Dataset/Training Set
### :bangbang: :hourglass_flowing_sand: Generating the question-answer pairs (assigned: Till, Tommy, Florian)
For each wiki, generate two datasets of the following type:
 - QQP-Dataset: `{(q⁺,q⁻,p) | for most passages p in the wiki}`
 - QPP-Dataset: `{(q,p⁺,p⁻) | for most passages p⁺ in the wiki}`
 
For the **first dataset** the questions can either be auto-generated by LLM (for example using the [gpt4free apis](https://github.com/xtekky/gpt4free), I also wrote a small test [script](retrieval/preprocessing/another_question_gen_script.py) for this) or semi-auto-generated, where each group member gets a small subset of the data (e.g. 100-200 passages or so) and has to create 1-3 questions that are answered by the passage and 1-3 questions that are not answered by it, either using ChatGPT or by coming up by themselves, etc.

For the **second dataset** the queries can be auto-generated by a LM for a given p⁺. We already have a script for this or again use a LLM like in the first dataset for this task. Finding passages that are not answered can be done by randomly sampling passages from the same/related/different wiki pages, or perhaps using TF-IDF on passages from the same page and selecting one of the least fitting passages.

Note that the task of generating the (q⁺, p) pair for the first dataset and (q, p⁺) from the second is the same, so you can recycle those results for the other dataset (whichever is generated first).

*NOTE: Not all passages have to be part of the training set, for example, we can only choose passages from wikis with a certain number of passages, or choose passages that are at least medium sized.*

### Storing the datasets
Each final datasets should then be saved in 4 files:
- `triples.tsv`:
    - triples are saved as followed: `QID⁺\tQID⁻\tPID\n`
    - QID⁽⁺⁻⁾=QueryID, PID=PassageID
    - QIDs and PIDs have to be generated an must be unique and consistend inside a dataset 
- `queries.tsv`:
    - queries are saved as followed: `QID\tquery\n`
- `passages.tsv`:
    - passages are saved as followed: `PID\tpassage\tWID\n`
    - WID = WikiID, which is the ID of the articel in the wiki
- `wiki.json`:
    - JSON file, which contains the meta information of each wiki-article ("id", "revid", "url", "title"),
    as well as the list of PIDs, which refer to the passages that are part of this article
    - format:
    ```json
    {
        "WID":
        {
            "revid": int,
            "url"  : string,
            "title": string,
            "PIDs" : list(int)
        },
        ...
    }
    ```

### Splitting into Training, Validation & Test Set
Split the datasets into a training, validation and test sets. \
The **training set** is used for training the neural IR models. \
The **validation set** is used to choose the best hyperparameters, such as which backbone to use, which dimensions the embedded vectors should have, what batch size to use, which similarity metric we should choose... \
The **test set** is then used to measure the performance of these models and our baseline model on unseen data. The separation of the validation and test sets is used to avoid overfitting our hyperparameters on the validation set and to avoid overly optimistic estimates of the model's performance.

The files should be saves as `triples.[train|val|test].tsv`.

It is probably better to split at a page level, so that some pages are part of the training set, some of the validation set and some of the test set;, rather than at the passage level, where passages from the same article may be in multiple sets.

It is extremely important that there is no overlap between the three data sets.

Try to find a good split ratio (80%-10%-10%, ...), search for typical ratios for similarly sized datasets.

### :hourglass_flowing_sand: Classes for loading the dataset (assigned: Aaron)
:hourglass_flowing_sand: Write a Python class for efficient loading to be capable of working with both of the dataset types.

The task is to create a Python class that takes the paths to the dataset files as input and can be used as either a *map-style* dataset or an *iterable-style* dataset. \
A *map-style* dataset works like a list, so given an index i, return the i-the triple.
An *iterable-style* iterates over the set of triples. \
At each iteration/index, the output should be either just the triple of IDs or the triple with the IDs replaces by the actual strings.
Maybe some inspiration can be found here: https://pytorch.org/docs/stable/data.html#dataset-types

The current [ColBERT dataloader](retrieval/data/dataloader.py) could be a good starting point, which implements an iterable-style dataset and also dataloader. One could base the dataset on this one, but maybe use better datastructures (maybe pandas.DataFrame). The tokenization step should be left out, as it should be implemented in each models dataloader. 

### :hourglass_flowing_sand: Benchmark datasets (assigned: Aaron)
Find benchmarks/ gold standard datasets, on which we can train our models and compare with other papers. (For example to make sure our implementation is correct or if we design our own model to be able to compare it with others) \
Look into: SQuAD 2.0, MS MARCO, TREC CAR... \
Choose a dataset with is similar to our task & dataset (I think MS MARCO should be similar to our QPP dataset)



## 3. Implement Models
### :white_check_mark: Baseline: BM-25 or TF-IDF (assigned: Till, Florian)
:heavy_check_mark: Implement the BM-25 or TF-IDF model, using an external library. \
:hourglass_flowing_sand: Use the dataset class for the BM-25 or TF-IDF model \
:exclamation: :white_check_mark: Implement efficient inference, so given a query find the best passages as fast as possible; maybe try to precompute the wiki passages? \
The implementation should work with the previous described datasets class. In case the output of the dataset class is not directly usable, you can write a dataloader, which for example tokenizes the data from the dataset class and then combines these into a batch of data, which is then directly feed into the model. I don't know if this is necessary tho. ^^

### :hourglass_flowing_sand: First Model: ColBERT (assigned: Aaron)
:heavy_check_mark: Implement the ColBERT model from the ColBERTv1 paper. \
:heavy_check_mark: Add support for other backbones, like RoBERTa, TinyBERT, etc. \
:heavy_check_mark: Write dataloaders base on the dataset class. \
:hourglass_flowing_sand: Write dataloaders base on the dataset class and PyTorch dataloader class. \
:white_check_mark: Implement Model/Tokenizer saving and loading. \
:hourglass_flowing_sand: Formulate the loss function, so that the training loop can just call .backward() on the loss. \
Implement efficient inference using re-ranking (requires efficient TF-IDF or BM-25 implementation)\
:white_check_mark: Implement efficient inference using full-retrieval.
Focus on inference performance ("model performance"/FLOPs, "model performance"/inference time [µs]) \
Try torch.compile() to improve runtime performance. \
Improve code quality (comments, typing, docstrings,...)

### :bangbang: Second Model: ???
Search for the code to the paper (e.g. https://paperswithcode.com/) or implement the model yourself using PyTorch (finding parameters would be very helpful for quicker training)

Other exotic approaches can be interesting (probably not big problem if it doesn't outperform baseline)


## 4. Training loop

### :hourglass_flowing_sand: Create a training script (assigned: Zhiwei)
Write a script for training the neural IR models. Have a look at the [ColBERT training script](https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/training/training.py) as an example.

It should use the dataset class for our datasets and the dataloader for the selected model. \
:hourglass_flowing_sand: Add [Learning-Rate-Schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Warmup & LR-decay). \
:hourglass_flowing_sand: Look into [AMP](https://pytorch.org/docs/stable/amp.html?highlight=amp#module-torch.amp) and maybe add AMP support. \
:hourglass_flowing_sand: Look into [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) training. It is important, that we get this working on the HPC, otherwise it is useless for us, so reading into the HPC would be necessary too.\
:hourglass_flowing_sand: Look into logging, either using [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) or [Weights&Biases](https://docs.wandb.ai/guides/integrations/pytorch). \
After each epoch, validate our model on the validation set using fitting evaluation metrics. \
The loss calculation should be part of the model, so we only need to call .backward() in the training loop. \
Implement Checkpoiting, where after a certain number of steps the model is saved.


### Train the models
*tba*



## 5. Evaluation 
### Metrics (assigned: Florian)
:white_check_mark: Implement metrics, like top-k accuracy, mean reciprocal rank, precision/recall, etc., which are suitable for our models and datasets. \
The metrics should use a fairly universal interface, so the outputs of the models can be easily converted into fitting data formats, that can interact with the metrics. \
Count the parameters in a model, meassure the FLOPs and ms per answer-retrieval.
(Parameters & FLOPs only necessary for neural IR approaches)

### Meassuring
Run the baseline and neural models on the test dataset and log their performance for later use in the paper. This script will probably look fairly similar to the training scripts.


## 6. Mockup
Demonstration of the model (done however you like; website, colab, application, ...)
Maybe some inference optimizations & pruning if the person in charge is interested in it and there is time


## 7. Paper
Final paper blablabla


## :checkered_flag:
