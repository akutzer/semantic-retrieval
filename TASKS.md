## :heavy_check_mark: 1. Fandom crawl
:heavy_check_mark: Download and extract fandom dumps. \
:heavy_check_mark: Clean up these dumps, i.e. remove wiki markdown syntax, remove tables, etc., using [WikiExtractor](https://github.com/attardi/wikiextractor)

:heavy_check_mark: Remove some pages, like the front page, and whatever else you can find. Update the passage RegEx pattern, which filters out passages, such as one of the type `__[\w*]__` or whatever.

:heavy_check_mark: Split the page content into paragraphs, by first splitting those paragraphs into sentences using sentence tokenizers. Then split the Sentences into words (using a [WordPieceTokenizer](https://huggingface.co/docs/tokenizers/api/models#tokenizers.models.WordPiece)) to get it's length. After that start merging the sentences until a certain maximal passage length is reached, then start a new passage. This passage length should be a hard limit, so no passage is longer than it. If a single sentence is longer than the maximum passage length, we have to split it in half.

:heavy_check_mark: If passages or paragraphs are very short, merge them together.

Save the final dump in a JSON with the following format:
```json
[
    {
        "id": "int",
        "revid": "int",
        "url": "string",
        "title": "string",
        "text": "list(string)"
    },
    ...
]
```

If we have time we might also download and extract multi-linguistic wikis.


## 2. :white_check_mark: Generate Dataset/Training Set
### :hourglass_flowing_sand: Generating the question-answer pairs (assigned: Till)
For each wiki, generate two datasets of the following type:
 - QQP-Dataset: `{(q⁺,q⁻,p) | for most passages p in the wiki}`
 - QPP-Dataset: `{(q,p⁺,p⁻) | for most passages p⁺ in the wiki}`
 
For the **first dataset** the questions can either be auto-generated by LLM (for example using the [gpt4free apis](https://github.com/xtekky/gpt4free), I also wrote a small test [script](retrieval/preprocessing/another_question_gen_script.py) for this) or semi-auto-generated, where each group member gets a small subset of the data (e.g. 100-200 passages or so) and has to create 1-3 questions that are answered by the passage and 1-3 questions that are not answered by it, either using ChatGPT or by coming up by themselves, etc.

For the **second dataset** the queries can be auto-generated by a LM for a given p⁺. We already have a script for this or again use a LLM like in the first dataset for this task. Finding passages that are not answered can be done by randomly sampling passages from the same/related/different wiki pages, or perhaps using TF-IDF on passages from the same page and selecting one of the least fitting passages.

Note that the task of generating the (q⁺, p) pair for the first dataset and (q, p⁺) from the second is the same, so you can recycle those results for the other dataset (whichever is generated first).

*NOTE: Not all passages have to be part of the training set, for example, we can only choose passages from wikis with a certain number of passages, or choose passages that are at least medium sized.*

### :heavy_check_mark: Storing the datasets
Each final datasets should then be saved in 4 files:
- `triples.tsv`:
    - triples are saved as followed: `QID⁺\tQID⁻\tPID\n`
    - QID⁽⁺⁻⁾=QueryID, PID=PassageID
    - QIDs and PIDs have to be generated an must be unique and consistend inside a dataset 
- `queries.tsv`:
    - queries are saved as followed: `QID\tquery\n`
- `passages.tsv`:
    - passages are saved as followed: `PID\tpassage\tWID\n`
    - WID = WikiID, which is the ID of the articel in the wiki
- `wiki.json`:
    - JSON file, which contains the meta information of each wiki-article ("id", "revid", "url", "title"),
    as well as the list of PIDs, which refer to the passages that are part of this article
    - format:
    ```json
    {
        "WID":
        {
            "revid": "int",
            "url"  : "string",
            "title": "string",
            "PIDs" : "list(int)"
        },
        ...
    }
    ```

### :heavy_check_mark: Splitting into Training, Validation & Test Set
Split the datasets into a training, validation and test sets. \
The **training set** is used for training the neural IR models. \
The **validation set** is used to choose the best hyperparameters, such as which backbone to use, which dimensions the embedded vectors should have, what batch size to use, which similarity metric we should choose... \
The **test set** is then used to measure the performance of these models and our baseline model on unseen data. The separation of the validation and test sets is used to avoid overfitting our hyperparameters on the validation set and to avoid overly optimistic estimates of the model's performance.

The files should be saves as `triples.[train|val|test].tsv`.

It is probably better to split at a page level, so that some pages are part of the training set, some of the validation set and some of the test set;, rather than at the passage level, where passages from the same article may be in multiple sets.

It is extremely important that there is no overlap between the three data sets.

Try to find a good split ratio (80%-10%-10%, ...), search for typical ratios for similarly sized datasets.

### :heavy_check_mark: Classes for loading the dataset (assigned: Aaron)
:heavy_check_mark: Write a Python class for efficient loading to be capable of working with both of the dataset types.

The task is to create a Python class that takes the paths to the dataset files as input and can be used as either a *map-style* dataset or an *iterable-style* dataset. \
A *map-style* dataset works like a list, so given an index i, return the i-the triple.
An *iterable-style* iterates over the set of triples. \
At each iteration/index, the output should be either just the triple of IDs or the triple with the IDs replaces by the actual strings.
Maybe some inspiration can be found here: https://pytorch.org/docs/stable/data.html#dataset-types

The current [ColBERT dataloader](retrieval/data/dataloader.py) could be a good starting point, which implements an iterable-style dataset and also dataloader. One could base the dataset on this one, but maybe use better datastructures (maybe pandas.DataFrame). The tokenization step should be left out, as it should be implemented in each models dataloader. 

### :heavy_check_mark: Benchmark datasets (assigned: Aaron)
Find benchmarks/ gold standard datasets, on which we can train our models and compare with other papers. (For example to make sure our implementation is correct or if we design our own model to be able to compare it with others) \
Look into: SQuAD 2.0, MS MARCO, TREC CAR... \
Choose a dataset with is similar to our task & dataset (I think MS MARCO should be similar to our QPP dataset)

### Structuring of the data directory

To ensure that all scripts work without problems, the structure of the data directory is defined as follows:
```
data/
├─ ms_marco/
│   ├─ ms_marco_v1_1/
│   │   ├─ train/
│   │   │   ├─ passages.tsv
│   │   │   ├─ queries.tsv
│   │   │   └─ triples.tsv
│   │   └─ val/
│   │       └─ ...
│   └─ ms_marco_v2_1/
│       ├─ train/
│       │   └─ ...
│       └─ val/
│           └─ ...
└─ fandoms_qa/
    ├─ harry_potter/
    │   ├─ train/
    │   │   ├─ passages.tsv
    │   │   ├─ queries.tsv
    │   │   ├─ triples.tsv
    │   │   └─ wiki.json
    │   ├─ val/
    │   │   └─ ...
    │   ├─ test/
    │   │   └─ ...
    │   ├─ human_verified/
    │   │   └─ ...
    │   └─ all/
    │       └─ ...
    ├─ *other fandom wikis*
    ...
    └─ fandoms_all/
        ├─ train/
        │   └─ ...
        ├─ val/
        │   └─ ...
        ├─ test/
        │   └─ ...
        ├─ human_verified/
        │   └─ ...
        └─ all/
            └─ ...
```

### Analyzing the datasets

Write a simple script which collects some basic statistics of the datasets.
Some examples would be:
- number of queries, passages, triples in the dataset
- avg./median number of words per query, passage
- distribution of question words (for MS MARCO v2.1 this is given in their paper)
- ...

The script should be run on MS MARCO v1.1 & v2.1 as well as on all the `human_verified/` and `all/` directories. Write down the results either in this markdown file or a google spreadsheet, so we can use them later in the final paper.




## 3. Implement Models
### :white_check_mark: Baseline: BM-25 or TF-IDF (assigned: Till, Florian)
:heavy_check_mark: Implement the BM-25 or TF-IDF model, using an external library. \
:hourglass_flowing_sand: Use the dataset class for the BM-25 or TF-IDF model \
:white_check_mark: Implement efficient inference, so given a query find the best passages as fast as possible; maybe try to precompute the wiki passages? \
The implementation should work with the previous described datasets class. In case the output of the dataset class is not directly usable, you can write a dataloader, which for example tokenizes the data from the dataset class and then combines these into a batch of data, which is then directly feed into the model. I don't know if this is necessary tho. ^^

### :hourglass_flowing_sand: Neural Retrieval Model: ColBERT (assigned: Aaron)
- :heavy_check_mark: Implement the ColBERT model from the ColBERTv1 paper.
- :heavy_check_mark: Add support for other backbones, like RoBERTa, TinyBERT, etc.
- :heavy_check_mark: Write dataloaders base on the dataset class.
- :heavy_check_mark: Write dataloaders base on the dataset class and PyTorch dataloader class.
- :white_check_mark: Implement Model/Tokenizer saving and loading.
- :hourglass_flowing_sand: Formulate the loss function, so that the training loop can just call .backward() on the loss.
- :hourglass_flowing_sand: Implement efficient inference using re-ranking (requires efficient TF-IDF or BM-25 implementation)
- :white_check_mark: Implement efficient inference using full-retrieval.
Focus on inference performance ("model performance"/FLOPs, "model performance"/inference time [µs])
- :white_check_mark: Improve code quality (comments, typing, docstrings,...)



## 4. Training

### :bangbang: :hourglass_flowing_sand: Create a training script (assigned: Zhiwei)
Write a script for training the neural IR models. Have a look at the [ColBERT training script](https://github.com/stanford-futuredata/ColBERT/blob/main/colbert/training/training.py) as an example.

- :heavy_check_mark: It should use the dataset class for our datasets and the dataloader for the selected model. 
- :hourglass_flowing_sand: Add [Learning-Rate-Schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Warmup & LR-decay). 
- :heavy_check_mark: Look into [AMP](https://pytorch.org/docs/stable/amp.html?highlight=amp#module-torch.amp) and maybe add AMP support. 
- :hourglass_flowing_sand: Look into [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) training. It is important, that we get this working on the HPC, otherwise it is useless for us, so reading into the HPC would be necessary too.
- :heavy_check_mark: Look into logging, either using [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) or [Weights&Biases](https://docs.wandb.ai/guides/integrations/pytorch). 
- :heavy_check_mark: After each epoch, validate our model on the validation set using fitting evaluation metrics. 
- :heavy_check_mark: Implement Checkpoiting, where after a certain number of steps the model is saved.
- :hourglass_flowing_sand: Implement model loading and resuming of training
- :hourglass_flowing_sand: Add CLI for the training script


### :bangbang: :hourglass_flowing_sand: Get HPC running (assigned: Tommy)
Maybe we should have a hpc directory in the repository where all the scripts for the HPC will be stored.

- :hourglass_flowing_sand: Move repository to the HPC 
- :hourglass_flowing_sand: Create and move all data to a data directory on the HPC (should be in the data/ directory and follow the above defined structure) 
- :bangbang: :hourglass_flowing_sand: Getting a training script on a single GPU running (i dont care what GPU it is, K80 could be enough for testing in the beginning) 
- (maybe: Getting a training script on multiple GPUs running)


### :bangbang: Train the models
**Roadmap:**
1. Validate the pretrained ColBERTv2 weights on MS MARCO v1.1 and v2.1 and compare them with the paper
2. Train our ColBERT implementation on MS MARCO (order of priority, not everything has to be done):
    1. ColBERT + MS MARCO v1.1
    2. ColBERT + MS MARCO v2.1
    3. ColRoBERTa + MS MARCO v1.1
    4. ColRoBERTa + MS MARCO v2.1 \
:arrow_right: decide if we should use BERT or RoBERTa as backbone
3. Train ColBERT on one Wiki of our choice and test different hyperparameters:
    - compare similarities: Normalized+Dot (=Cosine-Sim), Normalized+L2 and just L2
    - compare different embedding dims (maybe: 8, 16, 32, 64, 128)
4. Train ColBERT with the best set of hyperparameters (determined in step 3) on each wiki
5. Train ColBERT with the best set of hyperparameters on all wikis combined



## 5. Evaluation 
### :white_check_mark: Metrics (assigned: Florian)
:white_check_mark: Implement metrics, like top-k accuracy, mean reciprocal rank, precision/recall, etc., which are suitable for our models and datasets. \
The metrics should use a fairly universal interface, so the outputs of the models can be easily converted into fitting data formats, that can interact with the metrics. \
Count the parameters in a model, meassure the FLOPs and ms per answer-retrieval.
(Parameters & FLOPs only necessary for neural IR approaches)

### :bangbang: Meassuring (assigned: ???)
Run the baseline and neural models on the test dataset and log their performance for later use in the paper. This script will probably look fairly similar to the training scripts.

- Evaluate TF-IDF on MS MARCO and our Fandom datasets
    - for MS MARCO:
        - iterate over the validation triples and calculate MRR + top-1 & -3 accuracy + anything else you would like to meassure wrt. the given 10 passages
        - iterate over the validation triples and calculate MRR, accuracy, etc wrt. the entire passage.tsv file 
    - for FANDOM:
        - iterate over the validation triples and calculate MRR, accuracy, etc wrt. the entire passage.tsv file 
    - those are just some thoughts, you are free to do whatever you want, as long as it makes sense
- Evaluate pretrained ColBERTv2 on MS MARCO and our Fandom datasets
- Evaluate our trained models on MS MARCO and our Fandom datasets


### Model understanding (assigned: Florian)
1. :hourglass_flowing_sand: **Test if it's possible to extract roughly position of the answer.**
    - for example: query is encoded as 32 vectors. For each vector find the most similar passage vectors and visualize those 32 token in the passage string, does it correlate with the answer?
    - visualize the unsmoothed and smoothed (KDE or whatever) results 
    - can we make assumptions about how ColBERT might work?
2. **Find out what ColBERT is capable of doing that TF-IDF can't do**. Compare queries that ColBERT answered successfully but TF-IDF failed. Maybe you can find a pattern? Synonyms, ...?
3. **Find out what neither ColBERT nor TF-IDF can do**. Compare queries that both failed to answered. Maybe you can find a pattern?
4. **ColBERT is just context-unaware, synonym-robust embedding?**. Compare ColBERT embedding vs just its embedding matrix embedding  
5. Analyze the embedding space.
    - maybe some dimensionality reduction for a visualization
    - embedding space evenly used (**anisotropy**):
        > Recent work identifies an anisotropy problem in language representations (Ethayarajh, 2019; Li et al., 2020), i.e., the learned embeddings occupy a narrow cone in the vector space, which severely limits their expressiveness. Gao et al. (2019) demonstrate that language models trained with tied input/output embeddings lead to anisotropic word embeddings, and this is further observed by Ethayarajh (2019) in pre-trained contextual representations. Wang et al. (2020) show that singular values of the word embedding matrix in a language model decay drastically: except for a few dominating singular values, all others are close to zero.



## 6. Mockup
Demonstration of the model (done however you like; website, colab, application, ...)
Maybe some inference optimizations & pruning if the person in charge is interested in it and there is time


### 7. Presentation
blablabla buzzword blablabla

## 8. Paper
Final paper blablabla


## :checkered_flag:
