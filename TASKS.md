## 1. Fandom crawl (currently working: Tommy)
:white_check_mark: Download and extract fandom dumps. \
:white_check_mark: Clean up these dumps, i.e. remove wiki markdown syntax, remove tables, etc., using [WikiExtractor](https://github.com/attardi/wikiextractor)

:hourglass_flowing_sand: Remove some pages, like the front page, and whatever else you can find.

:hourglass_flowing_sand: Split the page content into paragraphs, by first splitting those paragraphs into sentences using sentence tokenizers and then merging them up to a certain passage length. If a single sentence is longer than the maximum passage length, we have to split it in half. If passages or paragraphs are very short, merge them together.

Save the final dump in a JSON with the following format:
```json
[
    {
        "id": int,
        "revid": int,
        "url": string,
        "title": string,
        "text": list(string)
    },
    ...
]
```

If we have time we might also download and extract multi-linguistic wikis.


## 2. Generate Dataset/Training Set
### Generating the question-answer pairs
For each wiki, generate two datasets of the following type:
 - QQP-Dataset: `{(q⁺,q⁻,p) | for most passages p in the wiki}`
 - QPP-Dataset: `{(q,p⁺,p⁻) | for most passages p⁺ in the wiki}`
 
For the **first dataset** the questions can either be auto-generated by LLM (for example using the `https://github.com/xtekky/gpt4free` apis) or semi-auto-generated, where each group member gets a small subset of the data (e.g. 100-200 passages or so) and has to create 1-3 questions that are answered by the passage and 1-3 questions that are not answered by it, either using ChatGPT or by comming up themselves, etc.

For the **second dataset** the queries can be auto-generated by a LM for a given p⁺. We already have a script for this or again use a LLM like in the first dataset for this task. Finding passages that are not answered can be done by randomly sampling passages from the same/related/different wiki pages, or perhaps using TF-IDF on passages from the same page and selecting one of the least fitting passages.

Note that the task of generating the (q⁺, p) pair for the first dataset and (q, p⁺) from the second is the same, so you can recycle those results for the other dataset (whichever is generated first).

*NOTE: Not all passages have to be part of the training set, for example, we can only choose passages from wikis with a certain number of passages, or choose passages that are at least medium sized.*

### Storing the datasets
Each final datasets should then be saved in 4 files:
- `triples.tsv`:
    - triples are saved as followed: `QID⁺\tQID⁻\tPID\n`
    - QID⁽⁺⁻⁾=QueryID, PID=PassageID
    - QIDs and PIDs have to be generated an must be unique and consistend inside a dataset 
- `queries.tsv`:
    - queries are saved as followed: `QID\tquery\n`
- `passages.tsv`:
    - passages are saved as followed: `PID\tpassage\tWID\n`
    - WID = WikiID, which is the ID of the articel in the wiki
- `wiki.json`:
    - JSON file, which contains the meta information of each wiki-article ("id", "revid", "url", "title"),
    as well as the list of PIDs, which refer to the passages that are part of this article
    - format:
    ```json
    "WID":
        "revid": int,
        "url"  : string,
        "title": string,
        "PIDs" : list(int),
    ```

### Splitting into Training, Validation & Test Set
Split the datasets into a training, validation and test sets. \
The **training set** is used for training the neural IR models. \
The **validation set** is used to choose the best hyperparameters, such as which backbone to use, which dimensions the embedded vectors should have, what batch size to use, which similarity metric we should choose... \
The **test set** is then used to measure the performance of these models and our baseline model on unseen data. The separation of the validation and test sets is used to avoid overfitting our hyperparameters on the validation set and to avoid overly optimistic estimates of the model's performance.

The files should be saves as `triples.[train|val|test].tsv`.

It is probably better to split at a page level, so that some pages are part of the training set, some of the validation set and some of the test set;, rather than at the passage level, where passages from the same article may be in multiple sets.

It is extremely important that there is no overlap between the three data sets.

Try to find a good split ratio (80%-10%-10%, ...), search for typical ratios for similarly sized datasets.

### Classes for loading the dataset
Write a Python class for efficient loading of these datasets.

The task is to create a Python class that takes the paths to the dataset files as input and can be used as either a map-style dataset or an iterable-style dataset. \
A *map-style* dataset works like a list, so given an index i, return the i-the triple.
An *iterable-style* iterates over the set of triples. \
At each iteration/index, the output should be either just the triple of IDs or the triple with the IDs replaces by the actual strings.
Maybe some inspiration can be found here: https://pytorch.org/docs/stable/data.html#dataset-types

The current [ColBERT dataloader](retrieval/data/dataloader.py) could be a good starting point, which implements an iterable-style dataset and also dataloader. One could base the dataset on this one, but maybe use better datastructures (maybe pandas.DataFrame). The tokenization step should be left out, as it should be implemented in each models dataloader. 


## Implement Models
### Baseline: BM-25 or TF-IDF
:white_check_mark: Implement the BM-25 or TF-IDF model, using an external library. \
The implementation should work with the previous described datasets class. In case the output of the dataset class is not directly usable, you can write a dataloader, which for example tokenizes the data from the dataset class and then combines these into a batch of data, which is then directly feed into the model. I don't know if this is necessary tho. ^^

### ColBERT
:white_check_mark: Implement the ColBERT model from the ColBERTv1 paper. \
:hourglass_flowing_sand: Add support for other backbones, like RoBERTa, TinyBERT, etc. \
:hourglass_flowing_sand: Write dataloaders base on the dataset class. \
Formulate the loss function, so that the training loop can just call .backward() on the loss. \
Implement efficient inference using either re-ranking or full-retrieval.

### other Model?
tba


## Training loop
Write a script for training the neural IR models.

It should use the dataset class for our datasets and the dataloader for the selected model. \
Add [Learning-Rate-Schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Warmup & LR-decay). \
Look into [AMP](https://pytorch.org/docs/stable/amp.html?highlight=amp#module-torch.amp) and maybe add AMP support. \
Look into [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) training. It is important, that we get this working on the HPC, otherwise it is useless for us, so reading into the HPC would be necessary too.\
Look into logging, either using [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) or [Weights&Biases](https://docs.wandb.ai/guides/integrations/pytorch). \
After each epoch, validate our model on the validation set using fitting evaluation metrics. \
The loss calculation should be part of the model, so we only need to call .backward() in the training loop.



## Evaluation
### Metrics
Implement metrics, like top-k accuracy, mean reciprocal rank, precision/recall, etc., which are suitable for our models and datasets. \
The metrics should use a fairly universal interface, so the outputs of the models can be easily converted into fitting data formats, that can interact with the metrics.

### Meassuring
Run the baseline and neural models on the test dataset and log their performance for later use in the paper. This script will probably look fairly similar to the training scripts.


## Mockup
tba


## Paper
tba
