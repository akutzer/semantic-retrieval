## 1. Fandom crawl:

Download and extract fandom wiki dumps. Clean up these dumps, i.e. remove wiki markdown syntax, etc.

Remove some pages, like the front page, and whatever else you can find.

Split the page content into paragraphs, by first splitting those paragraphs into sentences using sentence tokenizers and then merging them up to a certain passage length. If a single sentence is longer than the maximum passage length, we have to split it in half. If passages or paragraphs are very short, merge them together.

Save the final dump in a JSON with the following format:
```json
[
    {
       "id": int,
        "revid": int,
        "url": string,
        "title": string,
        "text": list(string)
    },
    ...
]
```


## 2. Generate Dataset/Training Set:

### Generating the question-answer pairs
For each wiki, from a selection of fandom wikis, generate 2 datasets of the following type:
 - `{(q⁺,q⁻,p) | for most passages p in the wiki}`
 - `{(q,p⁺,p⁻) | for most passages p⁺ in the wiki}`
 

For the **first dataset** the questions can either be auto-generated by LLM (for example using the `https://github.com/xtekky/gpt4free` apis) or semi-auto-generated, where each group member gets a small subset of the data (e.g. 100-200 passages or so) and has to create 1-3 questions that are answered by the passage and 1-3 questions that are not answered by it, either using ChatGPT or by creating them themselves, etc.

For the **second dataset** the queries can be auto-generated by a LM for a given p⁺. We already have a script for this or again use a LLM for it. Finding paths that are not answered can be done by randomly sampling passages from related wiki pages, or perhaps using TF-IDF on passages from the same page and selecting one of the least fitting passages. (However, this could falsify the performance of TF-IDF later on.)

Note that the task of generating the (q⁺, p) pair for the first dataset and (q, p⁺) from the second is the same, so you can recycle those results for the other dataset (whichever is generated first).

*NOTE: Not all passages have to be part of the training set, for example, we can only choose passages from wikis with a certain number of passages, or choose passages that are at least medium sized.*

### Storing the datasets
Each final datasets should then be saved in 4 files:
- `triples.tsv`:
    - triples are saved as followed: `QID⁺\tQID⁻\tPID\n`
    - QID⁽⁺⁻⁾ = QueryID, PID=PassageID
    - QIDs and PIDs have to be generated an must be unique and consistend inside a dataset 
- `queries.tsv`:
    - queries are saved as followed: `QID\tquery\n`
- `passages.tsv`:
    - passages are saved as followed: `PID\tpassage\tWID\n`
    - WID = WikiID, which is the ID of the articel in the wiki
- `wiki.json`:
    - JSON file, which contains the meta information of each wiki-article ("id", "revid", "url", "title"),
    as well as the list of PIDs, which refer to the passages, which are part of the article
    - format:
    ```json
    "WID":
        "revid": int,
        "url"  : string,
        "title": string,
        "pIDs" : list(int),
    ```

### Splitting into Training, Validation & Test Set

Split the datasets into a training, validation and test sets. \
The **training set** is used for training the neural IR models. \
The **validation set** is used to choose the best hyperparameters, such as which backbone to use, which dimensions the embedded vectors should have, what batch size to use, ... \
The **test set** is then used to measure the performance of these models and our baseline model on unseen data. The separation of the validation and test sets is used to avoid overfitting our hyperparameters on the validation set and to avoid overly optimistic estimates of the model's performance.

It is probably better to split at a page level, so that some pages are part of the training set, some of the validation set and some of the test set;, rather than at the passage level, where passages from the same article may be in multiple sets.

It is extremely important that there is no overlap between the three data sets.

Try to find a good split ratio (80%-10%-10%, ...), search for typical ratios for similarly sized datasets.

### Classes for loading the dataset
Write a Python class for efficient loading of datasets.

The task is to create a Python class that takes the paths to the dataset files as input and can be used as either a map-style dataset or an iterable-style dataset. \
A *map-style* dataset works like a list, so given an index i, return the i-the triple.
An *iterable-style* iterates over the set of triples. \
At each iteration/index, the output should be either just the triple or the triple,
with the IDs replaces with the actual strings.
Maybe some inspiration can be found here: https://pytorch.org/docs/stable/data.html#dataset-types



## Implement Models

### Baseline: BM-25 or TF-IDF
...


### ColBERT
Implement the ColBERT model from the ColBERTv1 paper. \
Add support for other backbones, like RoBERTa, TinyBERT, etc. \
Write dataloaders base on the dataset class. \
Write the loss function, so that the training loop can just call .backward() on the loss.


### other Model?


### Training loop
Write a script for training the neural IR models.

It should use the dataset class for our datasets, the dataloader for the selected model.\
Add [Learning-Rate-Schedulers](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) (Warmup & LR-decay). \
Look into [AMP](https://pytorch.org/docs/stable/amp.html?highlight=amp#module-torch.amp) and maybe add AMP support.
Look into [DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel) \
Look into logging, either using [Tensorboard](https://pytorch.org/docs/stable/tensorboard.html) or [Weights&Biases](https://docs.wandb.ai/guides/integrations/pytorch)
After each epoch, validate our model on the validation set



## Evaluation

### Metrics
Implement metrics.

### Meassuring
Run the baseline and neural models on the test dataset and log their performance for later use in the paper



## Mockup
tba


## Paper
tba
